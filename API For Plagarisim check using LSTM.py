# -*- coding: utf-8 -*-
"""Copy of NLPLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nd_F6GvZymiCd4-93A7gEDqjr35pCqmp
"""

import csv
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate, Dot, Lambda

"""Reading data"""

import csv
from tqdm.notebook import tqdm
import zipfile

# Path to the ZIP file
zip_file_path = '/content/train_snli.txt (1).zip'

# Extract and read data from the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as z:
    # Assuming the ZIP contains a single text file
    text_file_name = z.namelist()[0]  # Get the first file in the ZIP
    with z.open(text_file_name) as file:
        data = file.readlines()

# Prepare the CSV file
with open('data.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['source_txt', 'plagiarism_txt', 'label']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for line in tqdm(data):
        parts = line.decode('utf-8').strip().split('\t')  # Decode the binary line
        source_txt = parts[0]
        plagiarism_txt = parts[1]
        label = int(parts[2])

        writer.writerow({
            'source_txt': source_txt,
            'plagiarism_txt': plagiarism_txt,
            'label': label
        })

print('CSV file created successfully...')

import pandas as pd
import zipfile

# Path to the ZIP file
zip_file_path = '/content/train_snli.txt (1).zip'

# Extract and read the file from the ZIP
with zipfile.ZipFile(zip_file_path, 'r') as z:
    # Assuming the ZIP contains a single file
    text_file_name = z.namelist()[0]  # Get the first file name in the ZIP
    with z.open(text_file_name) as file:
        # Read the text file into a DataFrame
        df = pd.read_csv(file, sep='\t', header=None, names=['source_txt', 'plagiarism_txt', 'label'])


# Display the DataFrame
print(df.head())

df.shape

df.isnull().sum()

df.duplicated().sum()

df.dropna(inplace=True)

df.drop_duplicates(inplace=True)

print(df.isnull().sum())
print(df.duplicated().sum())
print(df.shape)

"""Bar Chart"""

df['label'].value_counts().plot(kind='bar')

print(df['source_txt'][1])
print(df['plagiarism_txt'][1])

"""Preprocessing Data"""

import string

def preprocess_text(text):
    if isinstance(text, str):
        # Remove punctuation
        text = text.translate(str.maketrans("", "", string.punctuation))
        # Convert to lowercase
        text = text.lower()
    return text

df['source_txt'] = df['source_txt'].apply(preprocess_text)
df['plagiarism_txt'] = df['plagiarism_txt'].apply(preprocess_text)

"""Tokenization"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['source_txt'] + '' + df['plagiarism_txt'])
X = tokenizer.texts_to_sequences(df['source_txt'] + '' + df['plagiarism_txt'])
X = pad_sequences(X)

y = df['label']

xtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2)

"""Build This Model"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example text data
texts = [
    "This is a sample text for testing",
    "Another example of text for sequence",
    "Deep learning with LSTM models"
]

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size

# Convert text to sequences and pad them
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, padding='post')  # Padded sequences

# Parameters for the model
embedding_dim = 100
input_length = X.shape[1]  # Length of input sequences

# Define the model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

# Force the model to build by providing a dummy input
model.build(input_shape=(None, input_length))

# Show the model summary
model.summary()

"""Train the Model"""

from keras.callbacks import EarlyStopping
callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Step 1: Tokenize and pad the sequences
texts = ["Your text data goes here", "Another example text", "Text data for training", "A different example text"]  # Example text data
labels = [0, 1, 0, 1]  # Example binary labels

tokenizer = Tokenizer(num_words=10000)  # Limit the vocabulary size
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=100)  # Pad to a maximum length

# Step 2: Convert to numpy arrays for model compatibility
padded_sequences = np.array(padded_sequences)  # Ensure it's a numpy array
labels = np.array(labels)  # Ensure labels are a numpy array

# Step 3: Split the data into training and test sets
xtrain, xtest = padded_sequences[:3], padded_sequences[3:]  # Adjust based on data
ytrain, ytest = labels[:3], labels[3:]

# Step 4: Define the model
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=100),
    LSTM(128, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Step 5: Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Step 6: Define early stopping
callback = EarlyStopping(monitor='val_loss', patience=2)

# Step 7: Train the model
history = model.fit(
    xtrain,
    ytrain,
    batch_size=64,
    epochs=1,
    validation_data=(xtest, ytest),
    callbacks=[callback]
)

print(history.history)

"""Creating A System To Predict Input Text"""

import pickle
model.save('model.h5')
with open('tokenizer.pkl','wb') as file:
    pickle.dump(tokenizer, file)

import tensorflow as tf
import zipfile
import os

# Step 1: Extract the contents of the .zip file
zip_file_path = '/content/train_snli.txt (1).zip'
extracted_folder = '/content/extracted_model/'

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

# List the extracted files to verify the content
extracted_files = os.listdir(extracted_folder)
print("Extracted files:", extracted_files)

# Step 2: Load the .txt file (assuming it contains SNLI data)
txt_file_path = os.path.join(extracted_folder, 'train_snli.txt')  # Update with actual filename
with open(txt_file_path, 'r') as file:
    lines = file.readlines()

# Example of how you could process the lines into data (simplified)
texts = []  # Text input data
labels = []  # Corresponding labels

# Process lines into pairs of sentences and labels (simplified assumption)
for line in lines:
    parts = line.strip().split('\t')
    if len(parts) > 1:
        sentence1, sentence2, label = parts[0], parts[1], parts[2]  # Example for SNLI format
        texts.append(sentence1 + " " + sentence2)  # Concatenate both sentences
        labels.append(int(label))  # Assuming labels are integers (0, 1, 2 for SNLI)

# Step 3: Tokenize the data
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=100)

# Step 4: Convert labels to one-hot encoding (for 3-class classification in SNLI)
labels = tf.keras.utils.to_categorical(labels, num_classes=3)

# Step 5: Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=128, input_length=100),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes for SNLI
])

# Step 6: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 7: Train the model
model.fit(padded_sequences, labels, batch_size=64, epochs=5)

# Step 8: Save the trained model
model.save('trained_snli_model.h5')

def pre_text(text):
    sequences = tokenizer.texts_to_sequences([text])
    padded_sequences = pad_sequences(sequences, maxlen = 111)
    return padded_sequences
def prdict_plagiarism(text):
    process_text = pre_text(text)
    predictions = model.predict(process_text)
    return predictions[0][0]

new_test1 = "My name is ravindra and i am from VIT"
new_test2 = "My name is Mahesh and i am from VIT"
predictions = prdict_plagiarism(new_test1)
predictions = prdict_plagiarism(new_test2)

if predictions >  .5 :

    print('This Text Has Plagiarism With Similarity Score IS:  ', predictions)
else:
    print("This Text Has No Plagiarism")

def pre_text(text):
    sequences = tokenizer.texts_to_sequences([text])
    padded_sequences = pad_sequences(sequences, maxlen = 111)
    return padded_sequences
def prdict_plagiarism(text):
    process_text = pre_text(text)
    predictions = model.predict(process_text)
    return predictions[0][0]

new_text = "a few people in a restaurant setting one of them is drinking orange juice"
new_text1 = "The people in a restaurant standing one the floor and drinking water"
predictions = prdict_plagiarism(new_text)
predictions = prdict_plagiarism(new_text1)

if predictions >  .5 :

    print('This Text Has Plagiarism With Similarity Score IS:  ', predictions)
else:
    print("This Text Has No Plagiarism")

df['plagiarism_txt'][50]

print(df['source_txt'][10])
print(df['plagiarism_txt'][10])